# Gradient descent for multiple linear regression


![[4.Gradient.webm]]

Summary of the transcript:

In this transcript, the speaker explains how to implement gradient descent for multiple linear regression with vectorization. The speaker starts by reviewing multiple linear regression and how it can be written using vector notation. The cost function is redefined as a function of the parameter vector w and the number b. The update rule for gradient descent is then shown for multiple features, where the parameters w are updated for each feature and b is updated as well. The speaker also introduces an alternative method called the normal equation for finding w and b for linear regression. However, this method is not generalized to other learning algorithms, and gradient descent is still a better way to implement linear regression. Finally, the speaker mentions an optional lab that demonstrates how to define a multiple regression model, calculate predictions, calculate the cost, and implement gradient descent using Python's NumPy library.

# optional-lab-multiple-linear-regression

![[C1_W2_Lab02.ipynb]]

