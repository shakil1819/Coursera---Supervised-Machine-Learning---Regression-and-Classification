<iframe src="https://d3c33hcgiwev3.cloudfront.net/C5Z_GpajT-qWfxqWo6_qsg.processed/full/540p/index.webm?Expires=1678665600&amp;Signature=WG5EcDQ2ObIqcQGBu89ZaVEoqvDWL5qzayd2~PWne-2REDHyueiI2r-21flxiw7eo2xz~YupoUAtS6dcAVL0KSDzhZKs0vG-RXe~9MtgW2V4BE2j1S4qCJe2v3i-irLPHzeFM9c7wVEPKl1HIQDtPKgf3VTNE2e-ugR3H0UY93M_&amp;Key-Pair-Id=APKAJLTNE6QMUY6HBC5A" allow="fullscreen" allowfullscreen="" style="height:100%;width:100%; aspect-ratio: 16 / 9; "></iframe>


#points 

| Points | Description |
| ------ | ----------- |
| 1      | Visualizations of the cost function j |
| 2      | Algorithm called gradient descent |
| 3      | Gradient descent applies to more general functions |
| 4      | Start off with some initial guesses for w and b |
| 5      | Keep on changing the parameters w and b to reduce the cost j of w, b |
| 6      | Some functions j may have more than one possible minimum |
| 7      | Gradient descent finds the direction of steepest descent |
| 8      | Repeat the process until you find yourself at the bottom of the valley |
| 9      | Gradient descent has an interesting property |

