<iframe src="https://d3c33hcgiwev3.cloudfront.net/aD0Fx6BsTue9BcegbG7noA.processed/full/540p/index.webm?Expires=1678665600&amp;Signature=izVmczkCwdvGW4Dqe5EwuPUFI5qAqNNg2EdIULRHYSMIto-d7sFviDH~WDI06xzbqkUSeds1U2kZlnUSStbO5JP0HfywU6MN6AL5klU5l0DUHjoz3fFael3~lNNWYpy7HeRwg~aG22GHXF5TTm8immNg~0v9J54wAPi66kuK6O4_&amp;Key-Pair-Id=APKAJLTNE6QMUY6HBC5A" allow="fullscreen" allowfullscreen="" style="height:100%;width:100%; aspect-ratio: 16 / 9; "></iframe>

#points 
Notes:

-   Gradient descent algorithm for updating parameters: w = old w - alpha * d/dw(J(wb))
-   Alpha is the learning rate, a small positive number between 0 and 1 that controls the size of the steps taken downhill.
-   The derivative term of the cost function J tells in which direction to take a step.
-   The equal sign can be used for assignment or truth assertion. In programming languages, truth assertions are sometimes written as ' = ='.
-   Model has two parameters: w and b, and both are updated simultaneously.
-   The correct way to implement gradient descent is to compute both updates for w and b and store them in temporary variables temp_w and temp_b. Then copy temp_w and temp_b to w and b, respectively.




