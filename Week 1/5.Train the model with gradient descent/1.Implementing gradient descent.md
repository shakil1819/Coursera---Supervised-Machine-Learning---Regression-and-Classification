![[Week 1/5.Train the model with gradient descent/attachments/index.mp4]]

#points 
Notes:

-   Gradient descent algorithm for updating parameters: w = old w - alpha * d/dw(J(wb))
-   Alpha is the learning rate, a small positive number between 0 and 1 that controls the size of the steps taken downhill.
-   The derivative term of the cost function J tells in which direction to take a step.
-   The equal sign can be used for assignment or truth assertion. In programming languages, truth assertions are sometimes written as ' = ='.
-   Model has two parameters: w and b, and both are updated simultaneously.
-   The correct way to implement gradient descent is to compute both updates for w and b and store them in temporary variables temp_w and temp_b. Then copy temp_w and temp_b to w and b, respectively.




