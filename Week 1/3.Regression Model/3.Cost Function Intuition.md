<iframe src="https://d3c33hcgiwev3.cloudfront.net/4cDgBYUORfCA4AWFDoXwuw.processed/full/540p/index.webm?Expires=1678665600&amp;Signature=ArS6acQm2LkioUury5Ccwj~BpbO-8E6BYPyNqHJAg1oE78LpwfWWX5NX~y-GvaaKlUbBmpvCjQbEeBvkHUOpqKgC8eE~FbCW7KA93N9PJxbBSVLdk~BLhh7I83Mmr2Z413atY2WJlYDqGCh~cQEW16WKPFEEvV3XPsC2eemk7Tg_&amp;Key-Pair-Id=APKAJLTNE6QMUY6HBC5A" allow="fullscreen" allowfullscreen="" style="height:100%;width:100%; aspect-ratio: 16 / 9; "></iframe>

#points 
Points from the transcript:

-   The video is about building intuition for the cost function.
-   The model is fw, b of x is w times x, plus b, where w and b are the parameters.
-   The cost function J measures the difference between the model's predictions and the actual true values for y.
-   The goal is to find values for w and b, so that the straight line fits the training data well.
-   The simplified model is fw of x, which is w times x, where b is equal to 0.
-   The goal with this simplified model is to find the value for w that minimizes J of w.
-   The cost function J is a function of w, where w controls the slope of the line defined by f w.
-   The cost defined by J depends on a parameter, in this case, the parameter w.
-   The cost function is the squared error cost function.
-   The cost function J is calculated for a particular value of w, say w is 1, and the error term inside the cost function is calculated using the data points.
-   When f(X^i) equals Y^i for each training example i, then f(X^i) minus Y^i is 0, and the cost J is equal to 0 for that particular value of w.
-   The cost function J can be plotted on the right, and the model f of x on the left.
#important
![[Pasted image 20230311114549.png]]


#### Question
![[Pasted image 20230311115643.png]]
![[Pasted image 20230311115727.png]]
